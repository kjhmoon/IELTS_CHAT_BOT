import sys
import os
import json
import warnings
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import Faithfulness, AnswerRelevancy
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

# 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • ë° ëª¨ë“ˆ import
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
rag_engine_path = os.path.join(project_root, '04_RAG_ENGINE')
sys.path.append(rag_engine_path)

from rag_modules import ConsultantAgent

# 2. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(os.path.join(project_root, '.env'))
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

def run_evaluation():
    # 3. ë°ì´í„°ì…‹ ë¡œë“œ
    with open('05_EVALUATE/test_dataset.json', 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    # 4. ë°ì´í„° ìˆ˜ì§‘ (ì§ˆë¬¸ -> ì±—ë´‡ -> ë‹µë³€/ë§¥ë½ ì €ì¥)
    questions = []
    answers = []
    contexts = []
    ground_truths = []

    print("ğŸš€ í‰ê°€ ë°ì´í„° ìƒì„± ì¤‘...")
    
    for item in raw_data:
        q = item['question']
        print(f"Processing: {q}")
        
        # ë§¤ë²ˆ ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ Agent ì¬ìƒì„±
        agent = ConsultantAgent()
        
        # with_context=Trueë¡œ ì„¤ì •í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê¹Œì§€ ë°›ìŒ
        response, retrieved_docs = agent.run(q, with_context=True)
        
        questions.append(q)
        answers.append(response)
        contexts.append(retrieved_docs)
        
        # Ground Truth ì²˜ë¦¬
        gt = item.get('notes', "") + " " + " ".join(item.get('check_points', []))
        
        # [ìˆ˜ì •] ë¦¬ìŠ¤íŠ¸([])ë¥¼ ì œê±°í•˜ê³  ë¬¸ìì—´(gt) ê·¸ëŒ€ë¡œ ì¶”ê°€
        ground_truths.append(gt) 

    # 5. RAGAS ë°ì´í„°ì…‹ í¬ë§· ë³€í™˜
    data_dict = {
        "user_input": questions,    # RAGAS ìµœì‹  ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ í‚¤ ì´ë¦„ ë³€ê²½ ê¶Œì¥ (question -> user_input)
        "response": answers,        # (answer -> response)
        "retrieved_contexts": contexts, # (contexts -> retrieved_contexts)
        "reference": ground_truths  # (ground_truth -> reference)
    }
    dataset = Dataset.from_dict(data_dict)

    # 6. í‰ê°€ ëª¨ë¸ ì„¤ì •
    gemini_llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash", 
        temperature=0,
        google_api_key=GEMINI_API_KEY
    )
    gemini_embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004", 
        google_api_key=GEMINI_API_KEY
    )

    # 7. í‰ê°€ ì‹¤í–‰
    print("\nğŸ“Š RAGAS í‰ê°€ ì‹œì‘ (ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)...")
    results = evaluate(
        dataset=dataset,
        metrics=[
            Faithfulness(),      # í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬
            AnswerRelevancy(),   # ì§ˆë¬¸ ê´€ë ¨ì„± ì²´í¬
        ],
        llm=gemini_llm,
        embeddings=gemini_embeddings
    )

    # 8. ê²°ê³¼ ì €ì¥
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")
    df = results.to_pandas()
    output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "ragas_report.xlsx")
    df.to_excel(output_path, index=False)
    print(f"ê²°ê³¼ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_evaluation()